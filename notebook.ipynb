{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m llm \u001b[39m=\u001b[39m OpenAI()\n\u001b[1;32m      5\u001b[0m chat \u001b[39m=\u001b[39m ChatOpenAI()\n\u001b[0;32m----> 7\u001b[0m a \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39;49m\u001b[39mHow many planets are there?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m b \u001b[39m=\u001b[39m chat\u001b[39m.\u001b[39mpredict(\u001b[39m\"\u001b[39m\u001b[39mHow many planets are there?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m a, b\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/langchain/llms/base.py:916\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 916\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[1;32m    878\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    879\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    880\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    881\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    882\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[1;32m    884\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/langchain/llms/openai.py:401\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    390\u001b[0m         {\n\u001b[1;32m    391\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         }\n\u001b[1;32m    399\u001b[0m     )\n\u001b[1;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    402\u001b[0m         \u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    404\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    405\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/langchain/llms/openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m/workspace/messwithLLM/env/lib/python3.11/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39;49msleep(seconds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "a = llm.predict(\"How many planets are there?\")\n",
    "b = chat.predict(\"How many planets are there?\")\n",
    "\n",
    "a, b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
